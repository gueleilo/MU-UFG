{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21fc12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   0%|          | 0/1000 [15:20<?, ? pairs/s]\n",
      "Creating augmented dataset:   0%|          | 0/1000 [15:05<?, ? pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path label\n",
      "0  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "1  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "2  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "3  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n",
      "4  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the training dataset\n",
    "dataset_path = './IRMAS/IRMAS-TrainingData'\n",
    "\n",
    "# List to store file paths and labels\n",
    "data = []\n",
    "\n",
    "# Iterate over each subfolder (instrument class) in the dataset\n",
    "for instrument_folder in os.listdir(dataset_path):\n",
    "    instrument_path = os.path.join(dataset_path, instrument_folder)\n",
    "    if not os.path.isdir(instrument_path):\n",
    "        continue  # Skip if not a directory\n",
    "    \n",
    "    # Extract instrument name from folder name\n",
    "    instrument_name = instrument_folder.split('(')[0]\n",
    "    # Iterate over files in the instrument folder\n",
    "    for file_name in os.listdir(instrument_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(instrument_path, file_name)\n",
    "            data.append({'file_path': file_path, 'label': instrument_name})\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c18b900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fluete', 'Cello', 'Piano', 'Clarinet', 'Organ ', 'Saxophone',\n",
       "       'Trumpet', 'Violin'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube = pd.read_csv('dataset_youtube.csv')\n",
    "df_youtube['classe'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8d2c88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['piano', 'organ', 'acoustic_guitar', 'electric_guitar', 'cello',\n",
       "       'clarinet', 'flute', 'saxophone', 'trumpet', 'violin'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_soundcloud = pd.read_csv('dataset_soundcloud.csv')\n",
    "df_soundcloud['classe'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77fa6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    'Fluete': 'flu',\n",
    "    'Cello': 'cel',\n",
    "    'Piano': 'pia',\n",
    "    'Clarinet': 'cla',\n",
    "    'Organ ': 'org',\n",
    "    'Saxophone': 'sax',\n",
    "    'Trumpet': 'tru',\n",
    "    'Violin': 'vio',\n",
    "    'piano': 'pia',\n",
    "    'organ': 'org',\n",
    "    'acoustic_guitar': 'gac',\n",
    "    'electric_guitar': 'gel',\n",
    "    'cello': 'cel',\n",
    "    'clarinet': 'cla',\n",
    "    'flute': 'flu',\n",
    "    'saxophone': 'sax',\n",
    "    'trumpet': 'tru',\n",
    "    'violin': 'vio'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffc0150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset:\n",
      "                                           file_path label\n",
      "0  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "1  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "2  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "3  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n",
      "4  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n"
     ]
    }
   ],
   "source": [
    "# Apply the mapping to the YouTube dataset\n",
    "df_youtube['label'] = df_youtube['classe'].map(mapping_dict)\n",
    "df_youtube = df_youtube.drop(columns=['classe'])\n",
    "df_youtube = df_youtube.rename(columns={'arquivo': 'file_path'})\n",
    "\n",
    "# Apply the mapping to the SoundCloud dataset\n",
    "df_soundcloud['label'] = df_soundcloud['classe'].map(mapping_dict)\n",
    "df_soundcloud = df_soundcloud.drop(columns=['classe'])\n",
    "df_soundcloud = df_soundcloud.rename(columns={'arquivo': 'file_path'})\n",
    "\n",
    "# Adjust file paths for YouTube and SoundCloud datasets\n",
    "df_youtube['file_path'] = './content/' + df_youtube['file_path']\n",
    "df_soundcloud['file_path'] = './content/' + df_soundcloud['file_path']\n",
    "\n",
    "# Concatenate the dataframes\n",
    "df_combined = pd.concat([df, df_youtube, df_soundcloud], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined dataset:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d60a9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: imblearn in /home/matheus.fares/.local/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/matheus.fares/.local/lib/python3.10/site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/matheus.fares/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.24.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d7de756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Função para balancear o dataset\n",
    "def balancear_dataset(df, estrategia='under'):\n",
    "    X = df['file_path'].values.reshape(-1, 1)  # Reshape para 2D\n",
    "    y = df['label']\n",
    "    \n",
    "    if estrategia == 'under':\n",
    "        sampler = RandomUnderSampler()\n",
    "    elif estrategia == 'over':\n",
    "        sampler = RandomOverSampler()\n",
    "    else:\n",
    "        raise ValueError(\"Estratégia deve ser 'under' ou 'over'\")\n",
    "\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "    \n",
    "    df_balanced = pd.DataFrame({\n",
    "        'file_path': X_res.flatten(),\n",
    "        'label': y_res\n",
    "    })\n",
    "    \n",
    "    return df_balanced\n",
    "# Balanceamento do dataset\n",
    "df_balanced = balancear_dataset(df_combined, estrategia='under')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "911017cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidades por classe no dataset balanceado:\n",
      "cel    778\n",
      "cla    778\n",
      "flu    778\n",
      "gac    778\n",
      "gel    778\n",
      "org    778\n",
      "pia    778\n",
      "sax    778\n",
      "tru    778\n",
      "vio    778\n",
      "voi    778\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "quantidade_por_classe = df_balanced['label'].value_counts()\n",
    "print(f\"Quantidades por classe no dataset balanceado:\\n{quantidade_por_classe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671323af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fb991d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   2%|▏         | 12/500 [00:00<00:04, 113.61 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1 MB | Elapsed time: 0.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:   5%|▍         | 24/500 [00:00<00:04, 100.32 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2 MB | Elapsed time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:   7%|▋         | 35/500 [00:00<00:04, 98.41 pairs/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3 MB | Elapsed time: 0.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:   9%|▉         | 45/500 [00:00<00:04, 97.07 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4 MB | Elapsed time: 0.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  11%|█         | 55/500 [00:00<00:04, 95.15 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5 MB | Elapsed time: 0.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  13%|█▎        | 65/500 [00:00<00:04, 96.08 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6 MB | Elapsed time: 0.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  17%|█▋        | 86/500 [00:00<00:04, 99.12 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7 MB | Elapsed time: 0.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  19%|█▉        | 96/500 [00:00<00:04, 97.35 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8 MB | Elapsed time: 0.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  21%|██        | 106/500 [00:01<00:04, 97.57 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9 MB | Elapsed time: 1.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  23%|██▎       | 116/500 [00:01<00:03, 97.37 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10 MB | Elapsed time: 1.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  25%|██▌       | 126/500 [00:01<00:03, 95.77 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 11 MB | Elapsed time: 1.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset:  27%|██▋       | 137/500 [00:01<00:03, 98.21 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12 MB | Elapsed time: 1.36 seconds\n",
      "Dataset size: 13 MB | Elapsed time: 1.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  33%|███▎      | 167/500 [00:01<00:03, 95.12 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14 MB | Elapsed time: 1.58 seconds\n",
      "Dataset size: 15 MB | Elapsed time: 1.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  37%|███▋      | 187/500 [00:01<00:03, 87.57 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 16 MB | Elapsed time: 1.82 seconds\n",
      "Dataset size: 17 MB | Elapsed time: 1.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  42%|████▏     | 208/500 [00:02<00:03, 92.69 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 18 MB | Elapsed time: 2.08 seconds\n",
      "Dataset size: 19 MB | Elapsed time: 2.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  46%|████▌     | 229/500 [00:02<00:02, 96.93 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 20 MB | Elapsed time: 2.30 seconds\n",
      "Dataset size: 21 MB | Elapsed time: 2.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  52%|█████▏    | 259/500 [00:02<00:02, 93.54 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 22 MB | Elapsed time: 2.53 seconds\n",
      "Dataset size: 23 MB | Elapsed time: 2.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  56%|█████▌    | 280/500 [00:02<00:02, 94.53 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 24 MB | Elapsed time: 2.76 seconds\n",
      "Dataset size: 25 MB | Elapsed time: 2.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  60%|██████    | 300/500 [00:03<00:02, 95.16 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 26 MB | Elapsed time: 2.99 seconds\n",
      "Dataset size: 27 MB | Elapsed time: 3.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  64%|██████▍   | 320/500 [00:03<00:02, 89.87 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 28 MB | Elapsed time: 3.23 seconds\n",
      "Dataset size: 29 MB | Elapsed time: 3.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  68%|██████▊   | 340/500 [00:03<00:01, 91.46 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 30 MB | Elapsed time: 3.48 seconds\n",
      "Dataset size: 31 MB | Elapsed time: 3.60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  72%|███████▏  | 360/500 [00:03<00:01, 89.38 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 32 MB | Elapsed time: 3.74 seconds\n",
      "Dataset size: 33 MB | Elapsed time: 3.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  78%|███████▊  | 390/500 [00:04<00:01, 89.59 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 34 MB | Elapsed time: 3.97 seconds\n",
      "Dataset size: 35 MB | Elapsed time: 4.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  82%|████████▏ | 409/500 [00:04<00:01, 89.43 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 36 MB | Elapsed time: 4.22 seconds\n",
      "Dataset size: 37 MB | Elapsed time: 4.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  86%|████████▌ | 430/500 [00:04<00:00, 93.92 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 38 MB | Elapsed time: 4.46 seconds\n",
      "Dataset size: 39 MB | Elapsed time: 4.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  90%|█████████ | 450/500 [00:04<00:00, 90.14 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 40 MB | Elapsed time: 4.68 seconds\n",
      "Dataset size: 41 MB | Elapsed time: 4.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  94%|█████████▍| 469/500 [00:05<00:00, 85.92 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 42 MB | Elapsed time: 4.94 seconds\n",
      "Dataset size: 43 MB | Elapsed time: 5.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  99%|█████████▉| 497/500 [00:05<00:00, 88.39 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 44 MB | Elapsed time: 5.21 seconds\n",
      "Dataset size: 45 MB | Elapsed time: 5.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating augmented dataset: 100%|██████████| 500/500 [00:05<00:00, 92.33 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of labels before augmentation:\n",
      "cel    778\n",
      "cla    778\n",
      "flu    778\n",
      "gac    778\n",
      "gel    778\n",
      "org    778\n",
      "pia    778\n",
      "sax    778\n",
      "tru    778\n",
      "vio    778\n",
      "voi    778\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Distribution of labels after augmentation:\n",
      "cel    880\n",
      "cla    872\n",
      "org    872\n",
      "pia    872\n",
      "voi    871\n",
      "vio    870\n",
      "tru    869\n",
      "sax    866\n",
      "flu    863\n",
      "gac    862\n",
      "gel    861\n",
      "dtype: int64\n",
      "                                              file_path    label     alpha\n",
      "0     ./IRMAS/IRMAS-TrainingData/cel/[cel][cla]0081_...      cel       NaN\n",
      "1     ./content/output_wav/Cello_Mischa Maisky plays...      cel       NaN\n",
      "2     ./content/output_wav/cello_Bach Suite for Cell...      cel       NaN\n",
      "3     ./IRMAS/IRMAS-TrainingData/cel/[cel][jaz_blu]0...      cel       NaN\n",
      "4     ./IRMAS/IRMAS-TrainingData/cel/074__[cel][nod]...      cel       NaN\n",
      "...                                                 ...      ...       ...\n",
      "9053  ./augdata_all/piagel[pia][pop_roc]1281__1_[gel...  pia,gel  0.995693\n",
      "9054  ./augdata_all/gelcelelectric_guitar_Práctica ...  gel,cel  0.057313\n",
      "9055  ./augdata_all/gacvoiacoustic_guitar_Acoustic G...  gac,voi  0.000048\n",
      "9056  ./augdata_all/saxgac[sax][cla]1725__2_[gac][cl...  sax,gac  0.937099\n",
      "9057  ./augdata_all/vioclaViolin_Tartini - _Devil's ...  vio,cla  0.658963\n",
      "\n",
      "[9058 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def audio_mixing_augmentation(df, output_dir, seed=42, sample_rate=16000, min_class_sample=1, max_augmented_size=None):\n",
    "    \"\"\"\n",
    "    Perform audio mixing data augmentation on a given DataFrame using MixUp technique.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'label' and 'file_path' columns.\n",
    "        output_dir (str): Directory where the augmented audio files will be saved.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        sample_rate (int): Sample rate for audio files.\n",
    "        min_class_sample (int): Minimum number of samples per class for balancing.\n",
    "        max_augmented_size (int or None): Maximum size of the augmented dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new augmented data.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    def mix_audios(audio1, audio2, alpha):\n",
    "        max_length = max(len(audio1), len(audio2))\n",
    "        padded_audio1 = np.pad(audio1, (0, max_length - len(audio1)), 'constant')\n",
    "        padded_audio2 = np.pad(audio2, (0, max_length - len(audio2)), 'constant')\n",
    "        mixed_audio = alpha * padded_audio1 + (1 - alpha) * padded_audio2\n",
    "        return mixed_audio\n",
    "\n",
    "    class_audio_dict = {label: df[df['label'] == label]['file_path'].tolist() for label in df['label'].unique()}\n",
    "    class_counts = df['label'].value_counts()\n",
    "\n",
    "    total_pairs_to_create = min(len(df), max_augmented_size) if max_augmented_size is not None else len(df)\n",
    "\n",
    "    augmented_data = []\n",
    "    pairs_created = {}\n",
    "    \n",
    "    # Minimum probability for any class (adjustable)\n",
    "    min_probability = 0.001\n",
    "    \n",
    "    # Calculate a weight based on class frequency (more frequent, lower weight)\n",
    "    class_weights = 1 / class_counts\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    total_weight = class_weights.sum()\n",
    "    normalized_weights = class_weights / total_weight\n",
    "    \n",
    "    # Amplify the effect for minority classes\n",
    "    amplified_weights = normalized_weights ** 2  # Amplify more significantly\n",
    "    \n",
    "    # Normalize amplified weights to sum to 1\n",
    "    amplified_weights /= amplified_weights.sum()\n",
    "    \n",
    "    # Calculate probabilities using amplified weights and minimum probability\n",
    "    class_probabilities = {label: max(min_probability, weight) for label, weight in zip(class_counts.index, amplified_weights)}\n",
    "\n",
    "    progress_bar = tqdm(total=total_pairs_to_create, desc=\"Creating augmented dataset\", unit=\" pairs\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_mb_printed = 0\n",
    "\n",
    "    # Balance the dataset by augmenting minor classes more frequently\n",
    "    while len(augmented_data) < total_pairs_to_create:\n",
    "        class1, class2 = np.random.choice(list(class_audio_dict.keys()), size=2, replace=False, p=list(class_probabilities.values()))\n",
    "\n",
    "        audio_paths1 = random.choices(class_audio_dict[class1], k=1)\n",
    "        audio_paths2 = random.choices(class_audio_dict[class2], k=1)\n",
    "\n",
    "        key = tuple(sorted((class1, class2)))\n",
    "        if key not in pairs_created:\n",
    "            pairs_created[key] = 0\n",
    "\n",
    "        audio1, sr1 = librosa.load(audio_paths1[0], sr=sample_rate)\n",
    "        audio2, sr2 = librosa.load(audio_paths2[0], sr=sample_rate)\n",
    "\n",
    "        alpha = np.random.beta(0.4, 0.4)\n",
    "        mixed_audio = mix_audios(audio1, audio2, alpha)\n",
    "        output_file_name = f\"{class1}{class2}{os.path.basename(audio_paths1[0]).split('.')[0]}_{os.path.basename(audio_paths2[0]).split('.')[0]}.wav\"\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        sf.write(output_file_path, mixed_audio, sample_rate)\n",
    "\n",
    "        augmented_data.append({'label': f\"{class1},{class2}\", 'file_path': output_file_path, 'alpha': alpha})\n",
    "        pairs_created[key] += 1\n",
    "\n",
    "        # Print progress and dataset size information\n",
    "        current_mb = sum(os.path.getsize(file['file_path']) for file in augmented_data) / (1024 * 1024)  # Size in MB\n",
    "        if int(current_mb) > last_mb_printed:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Dataset size: {int(current_mb)} MB | Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "            last_mb_printed = int(current_mb)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    combined_df = pd.concat([df, augmented_df]).reset_index(drop=True)\n",
    "\n",
    "    # Print distribution metrics\n",
    "    print(\"\\nDistribution of labels before augmentation:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(\"\\nDistribution of labels after augmentation:\")\n",
    "    labels = combined_df.label.apply(lambda x: x.split(\",\"))\n",
    "    flattened_labels = [label for sublist in labels for label in sublist]\n",
    "    \n",
    "    # Calculate value counts\n",
    "    value_counts = pd.Series(flattened_labels).value_counts()\n",
    "    print(value_counts)\n",
    "\n",
    "    return combined_df, df['label'].value_counts(), value_counts\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.DataFrame({\n",
    "#     'label': ['class1', 'class2', 'class1', 'class3'],\n",
    "#     'file_path': ['path/to/audio1.wav', 'path/to/audio2.wav', 'path/to/audio3.wav', 'path/to/audio4.wav']\n",
    "# })\n",
    "output_dir = './augdata_all'\n",
    "augmented_df, labels_before, labels_after = audio_mixing_augmentation(df_balanced, output_dir, seed=42, max_augmented_size=500)\n",
    "print(augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3e5a84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cel', 'cla', 'flu', 'gac', 'gel', 'org', 'pia', 'sax', 'tru',\n",
       "       'vio', 'voi', 'gel,voi', 'cla,cel', 'sax,cel', 'flu,gac',\n",
       "       'gac,pia', 'gel,org', 'org,pia', 'cla,sax', 'org,cel', 'sax,gac',\n",
       "       'flu,voi', 'flu,cel', 'flu,vio', 'org,cla', 'voi,tru', 'tru,sax',\n",
       "       'cel,gac', 'pia,gac', 'gac,tru', 'sax,tru', 'org,gel', 'cel,sax',\n",
       "       'vio,flu', 'vio,org', 'flu,gel', 'gel,flu', 'voi,gac', 'gac,voi',\n",
       "       'org,gac', 'pia,org', 'voi,flu', 'flu,tru', 'gac,flu', 'sax,cla',\n",
       "       'voi,cla', 'voi,vio', 'tru,pia', 'cla,vio', 'sax,vio', 'pia,sax',\n",
       "       'sax,flu', 'cla,gel', 'voi,gel', 'gac,cel', 'gel,vio', 'pia,voi',\n",
       "       'cla,pia', 'org,vio', 'tru,vio', 'org,tru', 'cel,vio', 'cel,pia',\n",
       "       'sax,voi', 'tru,flu', 'cel,voi', 'cla,flu', 'cel,cla', 'vio,sax',\n",
       "       'pia,cel', 'voi,pia', 'tru,gac', 'pia,vio', 'tru,voi', 'tru,cla',\n",
       "       'vio,gac', 'voi,org', 'gac,sax', 'voi,sax', 'cla,org', 'gac,vio',\n",
       "       'org,voi', 'gel,pia', 'gel,tru', 'pia,gel', 'gel,gac', 'gel,cla',\n",
       "       'cel,tru', 'flu,cla', 'gac,org', 'vio,voi', 'tru,cel', 'gel,cel',\n",
       "       'gac,cla', 'tru,gel', 'flu,sax', 'sax,org', 'vio,pia', 'sax,pia',\n",
       "       'cla,voi', 'sax,gel', 'cel,flu', 'org,sax', 'flu,pia', 'voi,cel',\n",
       "       'pia,flu', 'pia,tru', 'vio,cel', 'tru,org', 'cla,tru', 'gel,sax',\n",
       "       'vio,tru', 'cel,gel', 'vio,gel', 'cel,org', 'vio,cla', 'org,flu',\n",
       "       'pia,cla', 'flu,org', 'cla,gac'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2e4e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./IRMAS/IRMAS-TrainingData/cel/[cel][cla]0081_...</td>\n",
       "      <td>cel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./content/output_wav/Cello_Mischa Maisky plays...</td>\n",
       "      <td>cel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./content/output_wav/cello_Bach Suite for Cell...</td>\n",
       "      <td>cel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./IRMAS/IRMAS-TrainingData/cel/[cel][jaz_blu]0...</td>\n",
       "      <td>cel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./IRMAS/IRMAS-TrainingData/cel/074__[cel][nod]...</td>\n",
       "      <td>cel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9053</th>\n",
       "      <td>./augdata_all/piagel[pia][pop_roc]1281__1_[gel...</td>\n",
       "      <td>pia,gel</td>\n",
       "      <td>0.995693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9054</th>\n",
       "      <td>./augdata_all/gelcelelectric_guitar_Práctica ...</td>\n",
       "      <td>gel,cel</td>\n",
       "      <td>0.057313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9055</th>\n",
       "      <td>./augdata_all/gacvoiacoustic_guitar_Acoustic G...</td>\n",
       "      <td>gac,voi</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9056</th>\n",
       "      <td>./augdata_all/saxgac[sax][cla]1725__2_[gac][cl...</td>\n",
       "      <td>sax,gac</td>\n",
       "      <td>0.937099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>./augdata_all/vioclaViolin_Tartini - _Devil's ...</td>\n",
       "      <td>vio,cla</td>\n",
       "      <td>0.658963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9058 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path    label     alpha\n",
       "0     ./IRMAS/IRMAS-TrainingData/cel/[cel][cla]0081_...      cel       NaN\n",
       "1     ./content/output_wav/Cello_Mischa Maisky plays...      cel       NaN\n",
       "2     ./content/output_wav/cello_Bach Suite for Cell...      cel       NaN\n",
       "3     ./IRMAS/IRMAS-TrainingData/cel/[cel][jaz_blu]0...      cel       NaN\n",
       "4     ./IRMAS/IRMAS-TrainingData/cel/074__[cel][nod]...      cel       NaN\n",
       "...                                                 ...      ...       ...\n",
       "9053  ./augdata_all/piagel[pia][pop_roc]1281__1_[gel...  pia,gel  0.995693\n",
       "9054  ./augdata_all/gelcelelectric_guitar_Práctica ...  gel,cel  0.057313\n",
       "9055  ./augdata_all/gacvoiacoustic_guitar_Acoustic G...  gac,voi  0.000048\n",
       "9056  ./augdata_all/saxgac[sax][cla]1725__2_[gac][cl...  sax,gac  0.937099\n",
       "9057  ./augdata_all/vioclaViolin_Tartini - _Devil's ...  vio,cla  0.658963\n",
       "\n",
       "[9058 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1337cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py:2323: RuntimeWarning: invalid value encountered in cast\n",
      "  values = values.astype(str)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = './augmented_dataset_all.csv'\n",
    "\n",
    "# Salve o DataFrame como um arquivo CSV\n",
    "augmented_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a288e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10620d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
