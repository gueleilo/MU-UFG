{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fc12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path label\n",
      "0  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "1  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "2  ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...   cel\n",
      "3  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n",
      "4  ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...   cel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the training dataset\n",
    "dataset_path = './IRMAS/IRMAS-TrainingData'\n",
    "\n",
    "# List to store file paths and labels\n",
    "data = []\n",
    "\n",
    "# Iterate over each subfolder (instrument class) in the dataset\n",
    "for instrument_folder in os.listdir(dataset_path):\n",
    "    instrument_path = os.path.join(dataset_path, instrument_folder)\n",
    "    if not os.path.isdir(instrument_path):\n",
    "        continue  # Skip if not a directory\n",
    "    \n",
    "    # Extract instrument name from folder name\n",
    "    instrument_name = instrument_folder.split('(')[0]\n",
    "    # Iterate over files in the instrument folder\n",
    "    for file_name in os.listdir(instrument_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(instrument_path, file_name)\n",
    "            data.append({'file_path': file_path, 'label': instrument_name})\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb991d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def audio_mixing_augmentation(df, output_dir, seed=42, sample_rate=16000, min_class_sample=1, max_augmented_size=None):\n",
    "    \"\"\"\n",
    "    Perform audio mixing data augmentation on a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'label' and 'file_path' columns.\n",
    "        output_dir (str): Directory where the augmented audio files will be saved.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        sample_rate (int): Sample rate for audio files.\n",
    "        min_class_sample (int): Minimum number of samples per class for balancing.\n",
    "        max_augmented_size (int or None): Maximum size of the augmented dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new augmented data.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    def mix_audios(audio1, audio2):\n",
    "        max_length = max(len(audio1), len(audio2))\n",
    "        padded_audio1 = np.pad(audio1, (0, max_length - len(audio1)), 'constant')\n",
    "        padded_audio2 = np.pad(audio2, (0, max_length - len(audio2)), 'constant')\n",
    "        mixed_audio = (padded_audio1 + padded_audio2) / 2\n",
    "        return mixed_audio\n",
    "\n",
    "    class_audio_dict = {label: df[df['label'] == label]['file_path'].tolist() for label in df['label'].unique()}\n",
    "    class_counts = df['label'].value_counts()\n",
    "\n",
    "    total_pairs_to_create = min(len(df), max_augmented_size) if max_augmented_size is not None else len(df)\n",
    "\n",
    "    augmented_data = []\n",
    "    pairs_created = {}\n",
    "    \n",
    "    # Minimum probability for any class (adjustable)\n",
    "    min_probability = 0.001\n",
    "    \n",
    "    # Calculate a weight based on class frequency (more frequent, lower weight)\n",
    "    class_weights = 1 / (class_counts/ len(df))\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    total_weight = class_weights.sum()\n",
    "    normalized_weights = class_weights / total_weight\n",
    "    \n",
    "    # Amplify the effect for minority classes\n",
    "    # You can adjust the power factor (e.g., 1.5) to further increase the imbalance\n",
    "    amplified_weights = normalized_weights ** 0.5\n",
    "    \n",
    "    # Normalize amplified weights to sum to 1\n",
    "    amplified_weights /= amplified_weights.sum()\n",
    "    \n",
    "    # Calculate probabilities using amplified weights and minimum probability\n",
    "    class_probabilities = {label: max(min_probability, weight) for label, weight in zip(class_counts.index, amplified_weights)}\n",
    "\n",
    "    print(class_probabilities)\n",
    "\n",
    "    progress_bar = tqdm(total=total_pairs_to_create, desc=\"Creating augmented dataset\", unit=\" pairs\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_mb_printed = 0\n",
    "\n",
    "    # Balance the dataset by augmenting minor classes more frequently\n",
    "    while len(augmented_data) < total_pairs_to_create:\n",
    "        class1, class2 = np.random.choice(list(class_audio_dict.keys()), size=2, replace=False, p=list(class_probabilities.values()))\n",
    "\n",
    "        audio_paths1 = random.choices(class_audio_dict[class1], k=1)\n",
    "        audio_paths2 = random.choices(class_audio_dict[class2], k=1)\n",
    "\n",
    "        key = tuple(sorted((class1, class2)))\n",
    "        if key not in pairs_created:\n",
    "            pairs_created[key] = 0\n",
    "\n",
    "        audio1, sr1 = librosa.load(audio_paths1[0], sr=sample_rate)\n",
    "        audio2, sr2 = librosa.load(audio_paths2[0], sr=sample_rate)\n",
    "\n",
    "        mixed_audio = mix_audios(audio1, audio2)\n",
    "        output_file_name = f\"{class1}{class2}{os.path.basename(audio_paths1[0]).split('.')[0]}_{os.path.basename(audio_paths2[0]).split('.')[0]}.wav\"\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        sf.write(output_file_path, mixed_audio, sample_rate)\n",
    "\n",
    "        augmented_data.append({'label': f\"{class1},{class2}\", 'file_path': output_file_path})\n",
    "        pairs_created[key] += 1\n",
    "\n",
    "        # Print progress and dataset size information\n",
    "        current_mb = sum(os.path.getsize(file['file_path']) for file in augmented_data) / (1024 * 1024)  # Size in MB\n",
    "        if int(current_mb) > last_mb_printed:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Dataset size: {int(current_mb)} MB | Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "            last_mb_printed = int(current_mb)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    combined_df = pd.concat([df, augmented_df]).reset_index(drop=True)\n",
    "\n",
    "    # Print distribution metrics\n",
    "    print(\"\\nDistribution of labels before augmentation:\")\n",
    "    print(df['label'].value_counts(), df['label'].value_counts().mean(), df['label'].value_counts().std())\n",
    "    print(\"\\nDistribution of labels after augmentation:\")\n",
    "    labels = combined_df.label.apply(lambda x:x.split(\",\"))\n",
    "    flattened_labels = [label for sublist in labels for label in sublist]\n",
    "    \n",
    "    # Calculate value counts\n",
    "    value_counts = pd.Series(flattened_labels).value_counts()\n",
    "    print(value_counts, value_counts.mean(),  value_counts.std())\n",
    "    print(\"gain by class\", value_counts - df['label'].value_counts())\n",
    "\n",
    "    return combined_df, df['label'].value_counts(), value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e5a84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voi': 0.07918536221601388, 'gel': 0.08011759610640823, 'pia': 0.08225590288596382, 'org': 0.08457509977612815, 'gac': 0.08751147219219138, 'sax': 0.08827699468079771, 'vio': 0.0917108480446586, 'tru': 0.09194895536372252, 'cla': 0.09828540171872736, 'flu': 0.1040031361080633, 'cel': 0.11212923090732502}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   2%|▏         | 21/1000 [00:02<01:15, 13.00 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1 MB | Elapsed time: 2.49 seconds\n",
      "Dataset size: 2 MB | Elapsed time: 2.60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   5%|▌         | 52/1000 [00:02<00:22, 41.48 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3 MB | Elapsed time: 2.71 seconds\n",
      "Dataset size: 4 MB | Elapsed time: 2.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   7%|▋         | 72/1000 [00:03<00:15, 60.39 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5 MB | Elapsed time: 2.94 seconds\n",
      "Dataset size: 6 MB | Elapsed time: 3.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:   9%|▉         | 92/1000 [00:03<00:11, 75.85 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7 MB | Elapsed time: 3.16 seconds\n",
      "Dataset size: 8 MB | Elapsed time: 3.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  11%|█         | 112/1000 [00:03<00:10, 85.70 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9 MB | Elapsed time: 3.39 seconds\n",
      "Dataset size: 10 MB | Elapsed time: 3.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  13%|█▎        | 132/1000 [00:03<00:09, 91.30 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 11 MB | Elapsed time: 3.61 seconds\n",
      "Dataset size: 12 MB | Elapsed time: 3.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  15%|█▌        | 152/1000 [00:03<00:08, 94.49 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 13 MB | Elapsed time: 3.83 seconds\n",
      "Dataset size: 14 MB | Elapsed time: 3.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  18%|█▊        | 182/1000 [00:04<00:08, 95.56 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 15 MB | Elapsed time: 4.05 seconds\n",
      "Dataset size: 16 MB | Elapsed time: 4.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  20%|██        | 202/1000 [00:04<00:08, 95.48 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 17 MB | Elapsed time: 4.29 seconds\n",
      "Dataset size: 18 MB | Elapsed time: 4.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  22%|██▏       | 222/1000 [00:04<00:08, 94.63 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 19 MB | Elapsed time: 4.52 seconds\n",
      "Dataset size: 20 MB | Elapsed time: 4.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  24%|██▍       | 242/1000 [00:04<00:07, 95.05 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 21 MB | Elapsed time: 4.75 seconds\n",
      "Dataset size: 22 MB | Elapsed time: 4.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  26%|██▌       | 262/1000 [00:05<00:07, 94.41 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 23 MB | Elapsed time: 4.98 seconds\n",
      "Dataset size: 24 MB | Elapsed time: 5.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  29%|██▉       | 292/1000 [00:05<00:07, 95.25 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 25 MB | Elapsed time: 5.21 seconds\n",
      "Dataset size: 26 MB | Elapsed time: 5.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  31%|███       | 312/1000 [00:05<00:07, 94.24 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 27 MB | Elapsed time: 5.44 seconds\n",
      "Dataset size: 28 MB | Elapsed time: 5.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  33%|███▎      | 332/1000 [00:05<00:07, 94.49 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 29 MB | Elapsed time: 5.67 seconds\n",
      "Dataset size: 30 MB | Elapsed time: 5.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  35%|███▌      | 352/1000 [00:06<00:06, 93.44 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 31 MB | Elapsed time: 5.90 seconds\n",
      "Dataset size: 32 MB | Elapsed time: 6.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  37%|███▋      | 372/1000 [00:06<00:06, 94.09 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 33 MB | Elapsed time: 6.14 seconds\n",
      "Dataset size: 34 MB | Elapsed time: 6.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  39%|███▉      | 392/1000 [00:06<00:06, 94.09 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 35 MB | Elapsed time: 6.38 seconds\n",
      "Dataset size: 36 MB | Elapsed time: 6.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  42%|████▏     | 422/1000 [00:06<00:06, 93.12 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 37 MB | Elapsed time: 6.60 seconds\n",
      "Dataset size: 38 MB | Elapsed time: 6.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  44%|████▍     | 442/1000 [00:07<00:05, 93.63 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 39 MB | Elapsed time: 6.84 seconds\n",
      "Dataset size: 40 MB | Elapsed time: 6.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  46%|████▌     | 462/1000 [00:07<00:05, 93.55 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 41 MB | Elapsed time: 7.07 seconds\n",
      "Dataset size: 42 MB | Elapsed time: 7.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  48%|████▊     | 482/1000 [00:07<00:05, 93.50 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 43 MB | Elapsed time: 7.31 seconds\n",
      "Dataset size: 44 MB | Elapsed time: 7.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  50%|█████     | 502/1000 [00:07<00:05, 93.41 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 45 MB | Elapsed time: 7.54 seconds\n",
      "Dataset size: 46 MB | Elapsed time: 7.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  53%|█████▎    | 532/1000 [00:07<00:05, 92.04 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 47 MB | Elapsed time: 7.78 seconds\n",
      "Dataset size: 48 MB | Elapsed time: 7.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  55%|█████▌    | 552/1000 [00:08<00:04, 92.17 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 49 MB | Elapsed time: 8.01 seconds\n",
      "Dataset size: 50 MB | Elapsed time: 8.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  57%|█████▋    | 572/1000 [00:08<00:04, 92.23 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 51 MB | Elapsed time: 8.25 seconds\n",
      "Dataset size: 52 MB | Elapsed time: 8.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  59%|█████▉    | 592/1000 [00:08<00:04, 91.51 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 53 MB | Elapsed time: 8.49 seconds\n",
      "Dataset size: 54 MB | Elapsed time: 8.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  61%|██████    | 612/1000 [00:08<00:04, 91.37 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 55 MB | Elapsed time: 8.73 seconds\n",
      "Dataset size: 56 MB | Elapsed time: 8.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  63%|██████▎   | 632/1000 [00:09<00:04, 90.90 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 57 MB | Elapsed time: 8.97 seconds\n",
      "Dataset size: 58 MB | Elapsed time: 9.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  66%|██████▌   | 662/1000 [00:09<00:03, 91.14 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 59 MB | Elapsed time: 9.22 seconds\n",
      "Dataset size: 60 MB | Elapsed time: 9.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  68%|██████▊   | 682/1000 [00:09<00:03, 91.14 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 61 MB | Elapsed time: 9.44 seconds\n",
      "Dataset size: 62 MB | Elapsed time: 9.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  70%|███████   | 701/1000 [00:09<00:03, 89.15 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 63 MB | Elapsed time: 9.69 seconds\n",
      "Dataset size: 64 MB | Elapsed time: 9.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  72%|███████▏  | 720/1000 [00:10<00:03, 89.56 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 65 MB | Elapsed time: 9.94 seconds\n",
      "Dataset size: 66 MB | Elapsed time: 10.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  75%|███████▌  | 750/1000 [00:10<00:02, 90.39 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 67 MB | Elapsed time: 10.18 seconds\n",
      "Dataset size: 68 MB | Elapsed time: 10.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  77%|███████▋  | 770/1000 [00:10<00:02, 89.70 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 69 MB | Elapsed time: 10.43 seconds\n",
      "Dataset size: 70 MB | Elapsed time: 10.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  79%|███████▉  | 789/1000 [00:10<00:02, 89.54 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 71 MB | Elapsed time: 10.67 seconds\n",
      "Dataset size: 72 MB | Elapsed time: 10.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  81%|████████  | 807/1000 [00:11<00:02, 88.10 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 73 MB | Elapsed time: 10.91 seconds\n",
      "Dataset size: 74 MB | Elapsed time: 11.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  83%|████████▎ | 834/1000 [00:11<00:01, 83.49 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 75 MB | Elapsed time: 11.19 seconds\n",
      "Dataset size: 76 MB | Elapsed time: 11.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  85%|████████▌ | 852/1000 [00:11<00:01, 85.83 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 77 MB | Elapsed time: 11.44 seconds\n",
      "Dataset size: 78 MB | Elapsed time: 11.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  88%|████████▊ | 879/1000 [00:11<00:01, 86.61 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 79 MB | Elapsed time: 11.69 seconds\n",
      "Dataset size: 80 MB | Elapsed time: 11.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  90%|████████▉ | 897/1000 [00:12<00:01, 87.50 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 81 MB | Elapsed time: 11.94 seconds\n",
      "Dataset size: 82 MB | Elapsed time: 12.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  92%|█████████▏| 924/1000 [00:12<00:00, 86.96 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 83 MB | Elapsed time: 12.19 seconds\n",
      "Dataset size: 84 MB | Elapsed time: 12.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  94%|█████████▍| 942/1000 [00:12<00:00, 87.50 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 85 MB | Elapsed time: 12.45 seconds\n",
      "Dataset size: 86 MB | Elapsed time: 12.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  96%|█████████▌| 960/1000 [00:12<00:00, 87.32 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 87 MB | Elapsed time: 12.68 seconds\n",
      "Dataset size: 88 MB | Elapsed time: 12.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset:  99%|█████████▊| 987/1000 [00:13<00:00, 87.41 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 89 MB | Elapsed time: 12.94 seconds\n",
      "Dataset size: 90 MB | Elapsed time: 13.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset: 100%|██████████| 1000/1000 [00:13<00:00, 75.43 pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 91 MB | Elapsed time: 13.19 seconds\n",
      "\n",
      "Distribution of labels before augmentation:\n",
      "voi    778\n",
      "gel    760\n",
      "pia    721\n",
      "org    682\n",
      "gac    637\n",
      "sax    626\n",
      "vio    580\n",
      "tru    577\n",
      "cla    505\n",
      "flu    451\n",
      "cel    388\n",
      "Name: label, dtype: int64 609.5454545454545 125.21610410515385\n",
      "\n",
      "Distribution of labels after augmentation:\n",
      "voi    998\n",
      "gel    923\n",
      "pia    909\n",
      "org    852\n",
      "sax    819\n",
      "gac    792\n",
      "vio    791\n",
      "tru    771\n",
      "cla    685\n",
      "flu    612\n",
      "cel    553\n",
      "dtype: int64 791.3636363636364 133.79930696926104\n",
      "gain by class cel    165\n",
      "cla    180\n",
      "flu    161\n",
      "gac    155\n",
      "gel    163\n",
      "org    170\n",
      "pia    188\n",
      "sax    193\n",
      "tru    194\n",
      "vio    211\n",
      "voi    220\n",
      "dtype: int64\n",
      "                                              file_path    label\n",
      "0     ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...      cel\n",
      "1     ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...      cel\n",
      "2     ./IRMAS/IRMAS-TrainingData/cel/008__[cel][nod]...      cel\n",
      "3     ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...      cel\n",
      "4     ./IRMAS/IRMAS-TrainingData/cel/012__[cel][nod]...      cel\n",
      "...                                                 ...      ...\n",
      "7700  ./augdata/geltru031__[gel][dru][pop_roc]0974__...  gel,tru\n",
      "7701  ./augdata/voiflu125__[voi][dru][cou_fol]2405__...  voi,flu\n",
      "7702  ./augdata/piagac[pia][jaz_blu]1510__2_[gac][cl...  pia,gac\n",
      "7703  ./augdata/viopia[vio][jaz_blu]2117__2_[pia][ja...  vio,pia\n",
      "7704  ./augdata/saxvio[sax][cla]1614__3_[vio][cla]20...  sax,vio\n",
      "\n",
      "[7705 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = './augdata'\n",
    "augmented_df, labels_before, labels_after = audio_mixing_augmentation(df, output_dir, seed=42, max_augmented_size=1000)\n",
    "print(augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1337cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py:2323: RuntimeWarning: invalid value encountered in cast\n",
      "  values = values.astype(str)\n"
     ]
    }
   ],
   "source": [
    "output_csv_path = './augmented_dataset_all.csv'\n",
    "\n",
    "# Salve o DataFrame como um arquivo CSV\n",
    "augmented_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a288e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10620d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
